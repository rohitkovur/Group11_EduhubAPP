import { Injectable } from '@angular/core';
import { AuthService } from './auth.service';
import { ApiService } from './api.service';
import { environment } from '../../environments/environment';
import { Observable } from 'rxjs';
import { HttpParams } from '@angular/common/http';

export interface Article {
	id: string,
	createdBy: string,
	title: string,
	subtitle: string,
	imageUrl: string,
	description: string
}

@Injectable({
	providedIn: 'root'
})
export class ArticleService {

	public targetUrl = `${environment.api_url}article/`;

	articles: Article[] = [
		{
			id: '1',
			createdBy: 'User1',
			title: 'Sample Article 1',
			subtitle: 'Subtitle 1',
			imageUrl: 'https://images.pexels.com/photos/346529/pexels-photo-346529.jpeg?auto=compress&cs=tinysrgb&w=600',
			description: "<p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">It can take weeks to train a neural network on large datasets. Luckily, this time can be shortened thanks to model weights from pre-trained models – in other words, applying </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">transfer learning</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">.</span></span></p><p style=\"text-align:start\"><a href=\"https://machinelearningmastery.com/transfer-learning-for-deep-learning/\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Transfer learning </span></span></a><span style=\"background-color:rgb(17, 18, 19);\">is a technique that works in image classification tasks and natural language processing tasks. In this article, you’ll dive into:</span></p><ul><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">what transfer learning is,</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">how to implement transfer learning (in Keras),</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">transfer learning for image classification,</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">transfer learning for natural language processing</span></span></p></li></ul><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Well then, let’s start learning! (no pun intended… ok, maybe a little)&nbsp;</span></span></p><h2 style=\"text-align:start\"><strong><span style=\"color:rgb(255, 255, 255);\"><span style=\"background-color:rgb(17, 18, 19);\">What is transfer learning?</span></span></strong></h2><p style=\"text-align:start\"><a href=\"https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Transfer learning is about leveraging feature representations from a pre-trained model</span></span></a><span style=\"background-color:rgb(17, 18, 19);\">, so you don’t have to train a new model from scratch.&nbsp;</span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">The pre-trained models are usually trained on massive datasets that are a standard benchmark in the computer vision frontier. The weights obtained from the models can be reused in other computer vision tasks.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">These models can be used directly in making predictions on new tasks or integrated into the process of training a new model. Including the pre-trained models in a new model leads to lower training time and lower generalization error.&nbsp;&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Transfer learning is particularly very useful when you have a small training dataset. In this case, you can, for example, use the weights from the pre-trained models to </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">initialize the weights</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> of the new model. As you will see later, transfer learning can also be applied to natural language processing problems.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-idea.jpg?resize=768%2C432&amp;ssl=1\" alt=\"Transfer learning idea\" width=\"768\"></span></span></p><p><em><strong><span style=\"color:rgb(141, 145, 149);\">The idea of transfer learning</span></strong></em></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">The advantage of pre-trained models is that they are generic enough for use in other real-world applications. For example:</span></span></p><ul><li><p><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">models trained on the ImageNet</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> can be used in real-world image classification problems. This is because the dataset contains over 1000 classes. Let’s say you are an insect researcher. You can use these models and fine-tune them to classify insects.&nbsp;</span></span></p></li><li><p><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">classifying text requires knowledge of word representations</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> in some vector space. You can train vector representations yourself. The challenge here is that you might not have enough data to train the embeddings. Furthermore, training will take a long time. In this case, you can use a pre-trained word embedding like GloVe to hasten your development process.&nbsp;&nbsp;</span></span></p></li></ul><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">You will explore these use cases in a moment.</span></span></p><h3 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">What is the difference between transfer learning and fine-tuning?</span></span></strong></h3><p style=\"text-align:start\"><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Fine-tuning </span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">is an optional step in transfer learning. </span></span><a href=\"https://www.researchgate.net/post/What-is-the-difference-between-Transfer-Learning-vs-Fine-Tuning-vs-Learning-from-scratch\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Fine-tuning will usually improve the performance</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> of the model. However, since you have to retrain the entire model, you’ll likely overfit.&nbsp;</span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-vs-fine-tuning.png?ssl=1\" alt=\"\" width=\"699\"></span></span></p><p><a href=\"https://static-01.hindawi.com/articles/bmri/volume-2018/4605191/figures/4605191.fig.002.svgz\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a><a href=\"https://static-01.hindawi.com/articles/bmri/volume-2018/4605191/figures/4605191.fig.002.svgz\"><strong><span style=\"color:rgb(154, 164, 231);\">&nbsp;</span></strong></a></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Overfitting is avoidable. Just retrain the model or part of it using a </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">low learning rate</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. This is important because it prevents significant updates to the gradient. These updates result in poor performance. Using a callback to stop the training process when the model has stopped improving is also helpful.&nbsp;</span></span></p><h3 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Why use transfer learning?</span></span></strong></h3><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Assuming you have 100 images of cats and 100 dogs and want to build a model to classify the images. How would you train a model using this small dataset? You can train your model from scratch, but it will most likely overfit horribly. Enter transfer learning. Generally speaking, there are two big reasons why you want to use transfer learning:</span></span></p><ul><li><p><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">training models with high accuracy requires a lot of data</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. For example, the ImageNet dataset contains over 1 million images. In the real world, you are unlikely to have such a large dataset.&nbsp;</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">assuming that you had that kind of dataset, you might still </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">not have the resources required to train a model </span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">on such a large dataset. Hence transfer learning makes a lot of sense if you don’t have the compute resources needed to train models on huge datasets.&nbsp;</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">even if you had the compute resources at your disposal, you still have to</span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> wait for days or weeks to train such a model</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. Therefore using a pre-trained model will save you precious time.&nbsp;</span></span></p></li></ul><h3 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">When does transfer learning not work?</span></span></strong></h3><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Transfer learning will not work when the high-level features learned by the bottom layers are not sufficient to differentiate the classes in your problem. For example, a pre-trained model may be very good at identifying a door but not whether a door is closed or open. In this case, you can use the low-level features (of the pre-trained network) instead of the high-level features. In this case, you will have to retrain more layers of the model or use features from earlier layers.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">When datasets are not similar, features transfer poorly. This </span></span><a href=\"https://arxiv.org/pdf/1411.1792.pdf\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">paper</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> investigates the similarity of datasets in more detail. That said, as shown in the paper, initializing the network with pre-trained weights results in better performance than using random weights.&nbsp;</span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">You might find yourself in a situation where you consider the removal of some layers from the pre-trained model. Transfer learning is unlikely to work in such an event. This is because removing layers reduces the number of trainable parameters, which can result in overfitting.&nbsp; Furthermore, determining the correct number of layers to remove without overfitting is a cumbersome and time-consuming process.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-dogs.png?resize=434%2C244&amp;ssl=1\" alt=\"Transfer learning image\" width=\"388\"></span></span></p><p><a href=\"https://live.staticflickr.com/4544/38228876666_3782386ca7_b.jpg\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source&nbsp; &nbsp;</span></strong></em></a></p><p><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-spectrogram.png?resize=429%2C302&amp;ssl=1\" alt=\"Transfer learning image\" width=\"388\"></span></p><p><a href=\"https://upload.wikimedia.org/wikipedia/commons/2/2b/Oh_No_Girl_Spectrogram.jpg\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a></p><h2 style=\"text-align:start\"><strong><span style=\"color:rgb(255, 255, 255);\"><span style=\"background-color:rgb(17, 18, 19);\">How to implement transfer learning?</span></span></strong></h2><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Let’s now take a moment and look at how you can implement transfer learning.&nbsp;</span></span></p><h3 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Transfer learning in 6 steps</span></span></strong></h3><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">You can implement transfer learning in these six general steps.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-steps.png?ssl=1\" alt=\"Transfer learning steps\" width=\"808\"></span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-steps-2.png?ssl=1\" alt=\"Transfer learning steps\" width=\"808\"></span></span></p><p><a href=\"https://www.researchgate.net/profile/Maxime-Leclerc-2/publication/327023348/figure/fig1/AS:684562329919491@1540223786093/Transfer-learning-for-ship-classification-and-tracking-in-five-steps.png\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a></p><p style=\"text-align:start\"></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Obtain the pre-trained model</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">The first step is to get the </span></span><a href=\"https://keras.io/api/applications/\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">pre-trained model</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> that you would like to use for your problem. The various sources of pre-trained models are covered in a separate section.&nbsp;</span></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Create a base model</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Usually, the first step is to instantiate the </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">base mode</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">l using one of the architectures such as ResNet or Xception. You can also optionally download the </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">pre-trained weights</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. If you don’t download the weights, you will have to use the architecture to train your model from scratch. Recall that the base model will usually have more units in the final output layer than you require. When creating the base model, you, therefore, have to remove the final output layer. Later on, you will add a final output layer that is compatible with your problem.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Transfer-learning-base-model.jpg?resize=512%2C375&amp;ssl=1\" alt=\"Transfer learning base model\" width=\"512\"></span></span></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Freeze layers so they don’t change during training</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Freezing the layers from the pre-trained model is vital. This is because you don’t want the </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">weights in those layers to be re-initialized</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. If they are, then you will lose all the learning that has already taken place. This will be no different from training the model from scratch.&nbsp;</span></span></p><pre><code>base_model.trainable = False</code></pre><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Fine-tuning-pretrained-network.png?resize=584%2C321&amp;ssl=1\" alt=\"Fine tuning pretrained network\" width=\"584\"></span></span></p><p><a href=\"https://image.slidesharecdn.com/practicaldeeplearning-160329181459/95/practical-deep-learning-16-638.jpg?cb=1459275348\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Add new trainable layers&nbsp;</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">The next step is to </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">add new trainable layers</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> that will turn old features into predictions on the new dataset. This is important because the pre-trained model is loaded without the final output layer.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/New-trainable-layers.png?resize=581%2C494&amp;ssl=1\" alt=\"New trainable layers\" width=\"581\"></span></span></p><p><a href=\"https://pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_network_surgery.png\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Train the new layers on the dataset</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Remember that the pre-trained model’s final output will most likely be different from the output that you want for your model. For example, pre-trained models trained on the ImageNet dataset will output 1000 classes. However, your model might just have two classes. In this case, you have to train the model with a new output layer in place.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Therefore, you will add some new dense layers as you please, but most importantly, a final dense layer with </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">units corresponding to the number of outputs expected by your model</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">.&nbsp;</span></span></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Improve the model via fine-tuning</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Once you have done the previous step, you will have a model that can make predictions on your dataset. Optionally, you can improve its </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">performance through fine-tuning</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. Fine-tuning is done by unfreezing the base model or part of it and training the entire model again on the whole dataset at a very low learning rate. The low learning rate will increase the performance of the model on the new dataset while preventing overfitting.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">The learning rate has to be low because the model is quite large while the dataset is small. This is a recipe for overfitting, hence the low learning rate. Recompile the model once you have made these changes so that they can take effect. This is because the behavior of a model is frozen whenever you call the compile function. That means that you have to call the compile function again whenever you want to change the model’s behavior. The next step will be to train the model again while monitoring it via callbacks to ensure it does not overfit.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Freeze-layers.png?resize=580%2C497&amp;ssl=1\" alt=\"Freeze layers\" width=\"580\"></span></span></p><p><a href=\"https://pyimagesearch.com/wp-content/uploads/2019/06/fine_tuning_keras_freeze_unfreeze.png\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Pretty straightforward, eh?</span></span></p><h3 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Where to find pre-trained models?</span></span></strong></h3><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Let’s now talk about where you can find pre-trained models to use in your applications.&nbsp;</span></span></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Keras pre-trained models</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">There are more than two dozen pre-trained models available from Keras. They’re served via </span></span><a href=\"https://keras.io/api/applications/\" target=\"_blank\"><strong><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Keras applications</span></span></strong></a><strong><span style=\"background-color:rgb(17, 18, 19);\">.</span></strong><span style=\"background-color:rgb(17, 18, 19);\"> You get pre-trained weights alongside each model. When you download a model, the weights are downloaded automatically. They will be stored in `~/.keras/models/.` All the Keras applications are used for image tasks. For instance, here is how you can initialize the MobileNet architecture trained on ImageNet.&nbsp;</span></p><pre><code>model = tf.keras.applications.MobileNet(\n    input_shape=None,\n    alpha=1.0,\n    depth_multiplier=1,\n    dropout=0.001,\n    include_top=True,\n    weights=\"imagenet\",\n    input_tensor=None,\n    pooling=None,\n    classes=1000,\n    classifier_activation=\"softmax\",\n)</code></pre><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Transfer learning using TensorFlow Hub</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">It’s worth mentioning that Keras applications are not your only option for transfer learning tasks. You can also use models from </span></span><a href=\"https://www.tensorflow.org/hub\" target=\"_blank\"><strong><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">TensorFlow Hub</span></span></strong></a><span style=\"background-color:rgb(17, 18, 19);\">.</span></p><pre><code>model = tf.keras.Sequential([\n hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n                   trainable=False),\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n])</code></pre><h3><strong><span style=\"color:rgb(255, 255, 255);\"><span style=\"background-color:rgb(17, 18, 19);\">Related</span></span></strong></h3><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">See how you can track Keras model traning with </span></span><a href=\"https://docs.neptune.ai/integrations-and-supported-tools/model-training/tensorflow-keras\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Neptune’s integration with TensorFlow/Keras</span></span></a></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Pretrained word embeddings</span></span></strong></h4><p style=\"text-align:start\"><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Word embeddings </span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">are usually used for text classification problems. In as much as you can train your word embeddings, using a pre-trained one is much quicker. Here are a couple of word embeddings that you can consider for your natural language processing problems:</span></span></p><ul><li><p><a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">GloVe(Global Vectors for Word Representation) by Stanford</span></span></a></p></li><li><p><a href=\"https://code.google.com/archive/p/word2vec/\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Google’s Word2vec</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> trained on around 1000 billion words from Google News</span></p></li><li><p><a href=\"https://fasttext.cc/docs/en/english-vectors.html\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Fasttext</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> English vectors&nbsp;</span></p></li></ul><h3><strong><span style=\"color:rgb(255, 255, 255);\"><span style=\"background-color:rgb(17, 18, 19);\">READ ALSO</span></span></strong></h3><p><a href=\"https://neptune.ai/blog/word-embeddings-deep-dive-into-custom-datasets\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Training, Visualizing, and Understanding Word Embeddings: Deep Dive Into Custom Datasets</span></span></a></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Hugging face</span></span></strong></h4><p style=\"text-align:start\"><a href=\"https://github.com/huggingface\" target=\"_blank\"><strong><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Hugging Face</span></span></strong></a><span style=\"background-color:rgb(17, 18, 19);\"> provides thousands of pre-trained models for performing tasks on texts. Some of the supported functions include:</span></p><ul><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">question answering&nbsp;</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">summarization&nbsp;</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">translation and&nbsp;</span></span></p></li><li><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">text generation, to mention a few</span></span></p></li></ul><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Over 100 languages are supported by Hugging face.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Here’s an example of how you can use Hugging face to classify negative and positive sentences.&nbsp;</span></span></p><pre><code>from transformers import pipeline</code></pre><pre><code>classifier = pipeline('sentiment-analysis')\nclassifier('We are very happy to include pipeline into the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9978193640708923}]</code></pre><h3 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">How you can use pre-trained models</span></span></strong></h3><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">There are three ways to use a pre-trained model:</span></span></p><ul><li><p><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">prediction</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">,</span></span></p></li><li><p><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">feature extraction</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">,</span></span></p></li><li><p><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">fine-tuning</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">.</span></span></p></li></ul><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Prediction</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Here, you download the model and immediately use it to classify new images. </span></span><a href=\"https://keras.io/api/applications/resnet/#resnet50-function\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Here is an example of ResNet50</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> used to classify ImageNet classes.&nbsp;</span></p><p style=\"text-align:start\"><a href=\"https://en.wikipedia.org/wiki/ImageNet\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">ImageNet</span></span></a><span style=\"background-color:rgb(17, 18, 19);\"> is an extensive collection of images that have been used to train models, including ResNet50. There are over 1 million images and 1000 classes in this dataset.</span></p><pre><code>from tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\nmodel = ResNet50(weights='imagenet')\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]</code></pre><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Feature extraction</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">In this case, the output of the layer before the final layer is fed as input to a new model. The goal is to use the pre-trained model, or a part of it, to pre-process images and get essential features.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Then, you pass these features to a new classifier—no need to retrain the base model. The pre-trained convolutional neural network already has features that are important to the task at hand.&nbsp;</span></span></p><p><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(255, 255, 255);\"><img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Feature-extraction.png?resize=512%2C344&amp;ssl=1\" alt=\"Feature extraction\" width=\"512\"></span></span></p><p><a href=\"https://www.mdpi.com/applsci/applsci-10-03359/article_deploy/html/images/applsci-10-03359-g004b.png\" target=\"_blank\"><em><strong><span style=\"color:rgb(154, 164, 231);\">Source</span></strong></em></a></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">However, the pre-trained model’s final part doesn’t transfer over because it’s specific to its dataset. So, you have to build the last part of your model to fit your dataset.</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">In the natural language processing realm, pre-trained word embedding can be used for</span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> feature extraction.</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\"> The word embeddings help to place words in their right position in a vector space. They provide relevant information to a model because they can contextualize words in a sentence. The main objective of word embeddings is semantic understanding and the relationship between words. As a result, these word embeddings are task agnostic for natural language problems.&nbsp;</span></span></p><h4 style=\"text-align:start\"><strong><span style=\"color:rgb(222, 225, 228);\"><span style=\"background-color:rgb(17, 18, 19);\">Fine-tuning</span></span></strong></h4><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">When your new classifier is ready, you can use </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">fine-tuning to improve its accuracy</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">. To do this, you </span></span><strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">unfreeze the classifier</span></span></strong><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">, or part of it, and retrain it on new data with a low learning rate. Fine-tuning is critical if you want to make feature representations from the base model (obtained from the pre-trained model) more relevant to your specific task.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">You can also use weights from the pre-trained model to initialize weights in a new model. The best choice here depends on your problem, and you might need to experiment a bit before you get it right.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Still, there is a standard workflow you can use to apply transfer learning.&nbsp;</span></span></p><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">Let’s check it out.&nbsp;</span></span></p><h2 style=\"text-align:start\"><strong><span style=\"color:rgb(255, 255, 255);\"><span style=\"background-color:rgb(17, 18, 19);\">Example of transfer learning for images with Keras&nbsp;</span></span></strong></h2><p style=\"text-align:start\"><span style=\"color:rgb(186, 190, 195);\"><span style=\"background-color:rgb(17, 18, 19);\">With that background in place, let’s look at how you can use pre-trained models to solve image and text problems. Whereas there are many steps involved in training a model, the focus will be on those six steps specific to transfer learning.&nbsp;</span></span></p><h3><strong><span style=\"color:rgb(255, 255, 255);\"><span style=\"background-color:rgb(17, 18, 19);\">CHECK LATER</span></span></strong></h3><p><a href=\"https://docs.neptune.ai/integrations/tensorflow-keras.html\" target=\"_blank\"><span style=\"color:rgb(154, 164, 231);\"><span style=\"background-color:rgb(17, 18, 19);\">Neptune’s Integration With Keras</span></span></a></p><p><br></p>"
		},
		{
			id: '2',
			createdBy: 'vikram kumar',
			title: 'Sample Article 2',
			subtitle: 'Subtitle 2',
			imageUrl: 'https://images.pexels.com/photos/36717/amazing-animal-beautiful-beautifull.jpg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1',
			description: 'Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.'
		},

		// Add more sample articles as needed
	];
	user: any;

	constructor(private authService: AuthService, private apiService: ApiService) {
		this.authService.user$.subscribe(user => this.user = user);
	}

	createArticle(a: any): Observable<any> {
		a.id = a.title.replace(' ', '-') + '-' + this.makeid(5);
		a.createdBy = this.user.name;
		this.articles.push(a);

		return this.apiService.post(this.targetUrl + 'create')
	}

	editArticle(a: any, formValue: any) {
		a.title = formValue.title;
		a.subtitle = formValue.subtitle;
		a.description = formValue.description;
		a.imageUrl = formValue.imageUrl;

		// Find the index of the article with the specified ID
		const index = this.articles.findIndex(article => article.id === a.id);

		// If the article with the specified ID is found
		if (index !== -1) {
			// Replace the article at the found index with the new article
			this.articles[index] = a;
		} else {
			console.log('Article with ID ' + a.id + ' not found.');
		}
		return this.apiService.put(this.targetUrl + 'editArticle', formValue);
	}

	deleteArticle() {

	}

	getArticle(id: string): Article | undefined {
		return this.articles.find(article => article.id === id);
		// const params = new HttpParams()
		// 	.set('articleId', id);
		// return this.apiService.get(this.targetUrl+'viewArticle', params);
	}

	getArticlesList(mine: boolean = false) {
		// const params = new HttpParams()
		// 	.set('userId', this.user.userId);
		if (mine) {
			return this.articles.filter(a => a.createdBy == this.user.name);
			// return this.apiService.get(this.targetUrl+ 'allArticles');

		}
		return this.articles;
		// return this.apiService.get(this.targetUrl+ 'allArticles', params);
	}


	makeid(length: number) {
		let result = '';
		const characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';
		const charactersLength = characters.length;
		let counter = 0;
		while (counter < length) {
			result += characters.charAt(Math.floor(Math.random() * charactersLength));
			counter += 1;
		}
		return result;
	}
}
